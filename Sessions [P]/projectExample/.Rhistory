httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
handle <- readline('jorgemfassis')
handle
url_handle <- sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
url_handle
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
# whatever name you assigned to your created app
appname <- "biodiversity-sentiments"
## api key (example below is not a real key)
consumerkey <- "7Yqf5pfV1CYQKh5yTPlleKPJP"
consumersecret <- "0xVwkfuQZTv1yDImkzzSZRqBTddFhzkinHf9mezotwQqhDpWTk"
## api secret (example below is not a real key)
secret <- "49IV1toGY4amqS8ubbFN7RebfR3vljmIBVd1RvK97QArTuPnMv"
bearer <- "AAAAAAAAAAAAAAAAAAAAANKzYQEAAAAAXq%2FeFuhMWA%2BvxQYomO4hbaW17zQ%3DYCzyEmxtJ0oEuzaaXHwGAaRX7nyyMVg5hNnE4vUDMsI2CSpBBZ"
accesstoken <- "519688644-r8kG76aTAC72bWEdLS8ExrTXzuxiVb2xbs5ZVama"
accesssecret <- "Nu8e1VoJSVBm8J9zFLDtTiI98oeoUEkAF1ylnb09RIPqC"
twitter_token <- create_token(
app = appname,
consumer_key = consumerkey,
consumer_secret = consumersecret)
rstats_tweets <- search_tweets(q = "#rstats",n = 500)#, geocode="37.78,-122.40,1mi")
rstats_tweets <- search_tweets(q = "#rstats",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
bz <- lookup_coords("brazil")
rstats_tweets
rstats_tweets <- search_tweets(q = "#rstats",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
?search_tweets
rstats_tweets <- search_tweets(q = "realdonaldtrump",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
rstats_tweets
rstats_tweets <- search_tweets(q = "rstats",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
rstats_tweets
rstats_tweets <- search_tweets(q = "rstats",n = 500)#, geocode="37.78,-122.40,1mi")
twitter_token <- create_token(
app = appname,
consumer_key = consumerkey,
consumer_secret = consumersecret
,
access_token = accesstoken,
access_secret = accesssecret)
rstats_tweets <- search_tweets(q = "rstats",n = 500)#, geocode="37.78,-122.40,1mi")
appname
consumerkey
consumersecret
accesstoken
accesssecret
twitter_token <- create_token(
app = appname,
consumer_key = consumerkey,
consumer_secret = consumersecret,
access_token = accesstoken,
access_secret = accesssecret)
twitter_token
rstats_tweets <- search_tweets(q = "rstats",n = 500)#, geocode="37.78,-122.40,1mi")
rstats_tweets <- search_tweets(q = "rstats",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
twitter_token <- create_token(
app = appname,
consumer_key = consumerkey,
consumer_secret = consumersecret,
access_token = accesstoken,
access_secret = accesssecret)
rstats_tweets <- search_tweets(q = "rstats",n = 500, token=twitter_token)#, geocode="37.78,-122.40,1mi")
rstats_tweets
names(rstats_tweets)
rstats_tweets$text
rstats_tweets <- search_tweets(q = "",n = 500, token=twitter_token, geocode="37.78,-122.40,1mi")
rstats_tweets
rstats_tweets <- search_tweets(q = "",n = 500, token=twitter_token, geocode="37.043,-7.973,0.5mi")
rstats_tweets
rstats_tweets$text
rstats_tweets <- search_tweets(q = "",n = 500, token=twitter_token, geocode="37.043,-7.973,1mi")
rstats_tweets
rstats_tweets$text
rstats_tweets$hashtags
rstats_tweets$profile_image_url
rstats_tweets <- search_tweets(q = "",n = 500, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
catInHat = rstats_tweets$text
output = annotateString(catInHat)
output
output$sentiment
catInHat = rstats_tweets$text[3:4]
catInHat
output = annotateString(catInHat)
output$sentiment
catInHat
output = annotateString(catInHat[1])
output$sentiment
catInHat[1]
output = annotateString(catInHat[2])
output$sentiment
catInHat[2]
rstats_tweets <- search_tweets(q = "diving",n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets <- search_tweets(q = "diving",n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets$profile_image_url
catInHat = rstats_tweets$text
output = annotateString(catInHat[1])
output$sentiment
catInHat[1]
?annotateString
rstats_tweets <- search_tweets(q = "diving",   type = "all", n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets <- search_tweets(q = "diving",   type = "mixed", n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets <- search_tweets(q = "*",   type = "mixed", n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets <- search_tweets(q = ".",   type = "mixed", n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets <- search_tweets(type = "mixed", n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets <- search_tweets(n = 15000, token=twitter_token, geocode="38.4373,-9.1025,2mi")
rstats_tweets$text
rstats_tweets <- search_tweets(n = 15000, token=twitter_token, geocode="38.4373,-9.1025,20mi")
rstats_tweets$text
library(tidytext)
get_sentiments("afinn")
?get_sentiments
get_sentiments("bing")
get_sentiments("afinn")
install.packages('sentimentr')
library(devtools)
sentiment_by('I am not very good', by = NULL)
library('sentimentr')
library('sentimentr')
sentiment_by('I am not very good', by = NULL)
sentiment_by('I am very good', by = NULL)
rstats_tweets$text
rstats_tweets <- search_tweets(n = 15000, token=twitter_token, geocode="38.4373,-9.1025,10mi")
nrow(rstats_tweets)
rstats_tweets$text
rstats_tweets <- search_tweets(n = 100, token=twitter_token, geocode="37.863,-119.534,15mi")
rstats_tweets$text
sentiment_by(rstats_tweets$text, by = NULL)
sent <- sentiment_by(rstats_tweets$text, by = NULL)
sent$ave_sentiment
sent$ave_sentiment > 0.2
rstats_tweets$text[sent$ave_sentiment > 0.2]
rstats_tweets$text[sent$ave_sentiment < 0.2]
rstats_tweets$text[sent$ave_sentiment < 0.1]
sent
rstats_tweets$text[sent$ave_sentiment < 0]
rstats_tweets$text[sent$ave_sentiment > 0.5]
rstats_tweets$text[sent$ave_sentiment > 0.4]
rstats_tweets$location
rstats_tweets <- search_tweets(n = 1000, token=twitter_token, geocode="37.863,-119.534,10mi")
sent <- sentiment_by(rstats_tweets$text, by = NULL)
rstats_tweets$text[sent$ave_sentiment > 0.5]
rstats_tweets$text[sent$ave_sentiment > 0.25]
rstats_tweets$text[sent$ave_sentiment > 0.3]
rstats_tweets$text[sent$ave_sentiment < 0]
rstats_tweets$text[sent$ave_sentiment < -0.1]
rstats_tweets$text[sent$ave_sentiment < -0.3]
rstats_tweets$text[sent$ave_sentiment > 0.3]
rstats_tweets$text[sent$ave_sentiment < -0.3]
setwd("~/Dropbox/Tutoring/Classes & Courses/Ecological Niche Modelling and Climate Change/Main Contents/Sessions P/Recipes/A complete example")
# Load main functions
source("sourceFunctions.R")
# Read polygon defining global landmasses
world <- ne_countries(scale = 'medium')
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Paramuricea clavata")
# download the records from Obis
recordsObis <- getOccurrencesObis("Paramuricea clavata")
# open additional datasets with read.csv
recordsExternalFile <- read.csv("Data/Paramuricea_clavata.csv", sep=";")
# subset objects to get coordinates only
recordsGBIF <- recordsGBIF[,c("Lon","Lat")]
recordsObis <- recordsObis[,c("Lon","Lat")]
colnames(recordsExternalFile)
recordsExternalFile <- recordsExternalFile[,c("Lon","Lat")]
# test column names. If needed, change column names to allow rbind() function
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
# Change column names if needed
colnames(recordsExternalFile) <- c("Lon","Lat")
# merge datasets
records <- rbind(recordsGBIF,recordsObis,recordsExternalFile)
myExtent <- c(-20,50,20,60)
myRegion <- crop(world,extent(myExtent))
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# remove NA coordinates
records <- removeNA(records,"Lon","Lat")
# remove duplicate coordinates
records <- removeDuplicated(records,"Lon","Lat")
## based on a distance (km) to shore [alternative]
records <- removeOverLandDist(records, "Lon", "Lat", dist = 9)
# choose the region where the species occur
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
regionOfInterest <- drawPoly()
# select the records within the drawn polygon
pointsInRegion <- whichOverPolygon(records,regionOfInterest)
# clip the records of occurrence
records <- records[pointsInRegion,]
plot(myRegion,col="gray",border="gray")
points(records,pch=20, col="Black")
# remove records outside the known vertical distribution (example at 80m depth)
bathymetry <- raster("Data/BathymetryDepthMean.tif")
plot(bathymetry)
depthUse <- extract(bathymetry,records)
head(depthUse)
hist(depthUse,breaks=50)
records <- records[ which(depthUse > -250) ,]
depthUse <- extract(bathymetry,records)
hist(depthUse,breaks=50)
# plot records with plot function
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Clean distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# plot records with ggplot
ggplot() +
geom_polygon(data = myRegion, fill = "#B9B8B0", colour = "#707070", size = 0.2, aes(x = long, y = lat, group = group)) +
geom_point(data = records, aes(x = Lon, y = Lat), color = "#000000") +
scale_y_continuous(breaks = seq(-90,90, by=20)) +
scale_x_continuous(breaks = seq(-180,180,by=20)) +
coord_fixed() +
xlab("Longitude") + ylab("Latitude") + ggtitle("Clean distribution records")
# save data frame to external file
write.table(records,file="Data/mycleanRecords.csv",sep=";")
source("sourceFunctions.R")
# load clean occurrence data with two columns only for Lon and Lat (follow Recipe 1)
presences <- read.csv("Data/mycleanRecords.csv", sep = ";")
# load layers
layerCodes <- list_layers(datasets = "Bio-ORACLE")
# 02. load layers from hard disk [alternative]
list.files("Data/Present", full.names = T)
environmentalConditions <- stack(list.files("Data/Present", full.names = T))
# crop layers to the European extent
myExtent <- c(-20,47.5,10,60)
environmentalConditions <- crop(environmentalConditions,myExtent)
# plot predictors
plot(environmentalConditions)
# estimate collinearity between predictors [ > 0.85 can impact models negatively]
pairs(environmentalConditions)
# generate pseudo absences
pseudoAbs <- pseudoAbsences(environmentalConditions,presences,n=1000)
plot(pseudoAbs)
points(presences, col="red")
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
# Generate cross validation folds
folds <- getBlocks(modelData)
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
names(environmentalConditions)
monotonicity = data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=+1,
OceanTemperature_Benthic_Mean_Pred_LtMax=-1,
OceanTemperature_Benthic_Mean_Pred_LtMin=+1,
SeaWaterVelocity_Benthic_Mean_Pred_LtMin=+1,
TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
# given a set of possible hyperparameter values for BRT
h <- list(interaction.depth = c(1,2,3,4,5) , shrinkage = c(0.1,0.01,0.001) )
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
plot(exp1)
which.max(exp1@results$test_AUC)
exp1@results
exp1@results[which.max(exp1@results$test_AUC),]
# fit a BRT model to the dataset with the best hyperparameter values
model <- train("BRT", modelData, folds = folds , interaction.depth=1, shrinkage=0.01 )
getAUC(model, test = TRUE)
# determine relative variable contribution
viModel <- varImp(model, permut = 5)
plotVarImp(viModel)
# reduce model complexity by dropping one variable at a time
reducedModel <- reduceVar(model, th = 5, metric = "auc", permut = 5)
# determine the final performance as AUC
getAUC(reducedModel, test = TRUE)
# determine the final relative variable contribution
viModel <- varImp(reducedModel, permut = 5)
viModel
plotVarImp(viModel)
# inspect response curves
names(environmentalConditions)
plotResponse(reducedModel, var = "OceanTemperature_Benthic_Mean_Pred_LtMin", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
plotResponse(reducedModel, var = "TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
plotResponse(reducedModel, var = "OceanTemperature_Benthic_Mean_Pred_LtMax", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
# predict with BRT to raster stack
mapPresent <- predict(reducedModel, environmentalConditions, type=c("logistic"))
plot(mapPresent)
# determine the threshold maximizing the sum of sensitivity and specificity
threshold <- thresholdMaxTSS(reducedModel)
threshold
# generate a reclassification table
thresholdConditions <- data.frame(from = c(0,threshold) , to=c(threshold,1) , reclassValue=c(0,1))
thresholdConditions
# apply threshold to reclassify the predictive surface
mapPresentReclass <- reclassify(mapPresent, rcl = thresholdConditions)
plot(mapPresentReclass)
# determine model performance
getAccuracy(reducedModel,threshold = threshold)
getAUC(reducedModel, test = TRUE)
# load layers of the future
environmentalConditionsRCP26 <- stack(list.files("Data/Future RCP26", full.names = T))
# crop layers to the European extent
environmentalConditionsRCP26 <- crop(environmentalConditionsRCP26,myExtent)
# predict with BRT to raster stack
mapRCP26 <- predict(reducedModel, environmentalConditionsRCP26, type=c("logistic"))
plot(mapRCP26)
plot(mapRCP26 - mapPresent)
# apply threshold to reclassify the future predictive surface
mapRCP26Reclass <- reclassify(mapRCP26, rcl = thresholdConditions)
plot(mapRCP26Reclass)
# load layers of the future
environmentalConditionsRCP85 <- stack(list.files("Data/Future RCP85", full.names = T))
# crop layers to the European extent
environmentalConditionsRCP85 <- crop(environmentalConditionsRCP85,myExtent)
# predict with BRT to raster stack
mapRCP85 <- predict(reducedModel, environmentalConditionsRCP85, type=c("logistic"))
plot(mapRCP85)
plot(mapRCP85 - mapPresent)
# apply threshold to reclassify the future predictive surface
mapRCP85Reclass <- reclassify(mapRCP85, rcl = thresholdConditions)
plot(mapRCP85Reclass)
par(mfrow=c(1,3))
plot(mapPresent)
plot(mapRCP26 - mapPresent)
plot(mapRCP85 - mapPresent)
par(mfrow=c(1,3))
plot(mapPresentReclass)
plot(mapRCP26Reclass)
plot(mapRCP85Reclass)
dev.off()
getwd()
getwd()
# Load main functions
source("sourceFunctions.R")
# Read polygon defining global landmasses
world <- ne_countries(scale = 'medium')
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Paramuricea clavata")
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Paramuricea clavata")
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Paramuricea clavata")
# download the records from Obis
recordsObis <- getOccurrencesObis("Paramuricea clavata")
# open additional datasets with read.csv
recordsExternalFile <- read.csv("Data/Paramuricea_clavata.csv", sep=";")
# subset objects to get coordinates only
recordsGBIF <- recordsGBIF[,c("Lon","Lat")]
recordsObis <- recordsObis[,c("Lon","Lat")]
colnames(recordsExternalFile)
recordsExternalFile <- recordsExternalFile[,c("Lon","Lat")]
# test column names. If needed, change column names to allow rbind() function
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
# merge datasets
records <- rbind(recordsGBIF,recordsObis,recordsExternalFile)
myExtent <- c(-20,50,20,60)
myRegion <- crop(world,extent(myExtent))
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# remove NA coordinates
records <- removeNA(records,"Lon","Lat")
# remove duplicate coordinates
records <- removeDuplicated(records,"Lon","Lat")
## based on a distance (km) to shore [alternative]
records <- removeOverLandDist(records, "Lon", "Lat", dist = 9)
# choose the region where the species occur
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
regionOfInterest <- drawPoly()
# select the records within the drawn polygon
pointsInRegion <- whichOverPolygon(records,regionOfInterest)
# clip the records of occurrence
records <- records[pointsInRegion,]
plot(myRegion,col="gray",border="gray")
points(records,pch=20, col="Black")
# remove records outside the known vertical distribution (example at 80m depth)
bathymetry <- raster("Data/BathymetryDepthMean.tif")
plot(bathymetry)
depthUse <- extract(bathymetry,records)
head(depthUse)
hist(depthUse,breaks=50)
View(records)
records <- records[ which(depthUse > -250) ,]
depthUse <- extract(bathymetry,records)
hist(depthUse,breaks=50)
# plot records with plot function
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Clean distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# plot records with ggplot
ggplot() +
geom_polygon(data = myRegion, fill = "#B9B8B0", colour = "#707070", size = 0.2, aes(x = long, y = lat, group = group)) +
geom_point(data = records, aes(x = Lon, y = Lat), color = "#000000") +
scale_y_continuous(breaks = seq(-90,90, by=20)) +
scale_x_continuous(breaks = seq(-180,180,by=20)) +
coord_fixed() +
xlab("Longitude") + ylab("Latitude") + ggtitle("Clean distribution records")
# save data frame to external file
write.table(records,file="Data/mycleanRecords.csv",sep=";")
# load clean occurrence data with two columns only for Lon and Lat (follow Recipe 1)
presences <- read.csv("Data/mycleanRecords.csv", sep = ";")
# 02. load layers from hard disk [alternative]
list.files("Data/Present", full.names = T)
environmentalConditions <- stack(list.files("Data/Present", full.names = T))
list.files("Data/Present", full.names = T)
# crop layers to the European extent
myExtent <- c(-20,47.5,10,60)
environmentalConditions <- crop(environmentalConditions,myExtent)
# plot predictors
plot(environmentalConditions)
# estimate collinearity between predictors [ > 0.85 can impact models negatively]
pairs(environmentalConditions)
# generate pseudo absences
pseudoAbs <- pseudoAbsences(environmentalConditions,presences,n=1000)
plot(pseudoAbs)
points(presences, col="red")
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
# Generate cross validation folds
folds <- getBlocks(modelData)
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
names(environmentalConditions)
monotonicity = data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=+1,
OceanTemperature_Benthic_Mean_Pred_LtMax=-1,
OceanTemperature_Benthic_Mean_Pred_LtMin=+1,
SeaWaterVelocity_Benthic_Mean_Pred_LtMin=+1,
TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
# given a set of possible hyperparameter values for BRT
h <- list(interaction.depth = c(1,2,3,4,5) , shrinkage = c(0.1,0.01,0.001) )
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
plot(exp1)
exp1@results
View(exp1@results)
which.max(exp1@results$test_AUC)
exp1@results[which.max(exp1@results$test_AUC),]
# fit a BRT model to the dataset with the best hyperparameter values
model <- train("BRT", modelData, folds = folds , interaction.depth=1, shrinkage=0.01 )
getAUC(model, test = TRUE)
# determine relative variable contribution
viModel <- varImp(model, permut = 5)
plotVarImp(viModel)
# reduce model complexity by dropping one variable at a time
reducedModel <- reduceVar(model, th = 5, metric = "auc", permut = 5)
# determine the final performance as AUC
getAUC(reducedModel, test = TRUE)
# determine the final relative variable contribution
viModel <- varImp(reducedModel, permut = 5)
viModel
plotVarImp(viModel)
# inspect response curves
names(environmentalConditions)
plotResponse(reducedModel, var = "DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
# inspect response curves
names(environmentalConditions)
plotResponse(reducedModel, var = "OceanTemperature_Benthic_Mean_Pred_LtMin", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
# predict with BRT to raster stack
mapPresent <- predict(reducedModel, environmentalConditions, type=c("logistic"))
plot(mapPresent)
# determine the threshold maximizing the sum of sensitivity and specificity
threshold <- thresholdMaxTSS(reducedModel)
threshold
# generate a reclassification table
thresholdConditions <- data.frame(from = c(0,threshold) , to=c(threshold,1) , reclassValue=c(0,1))
thresholdConditions
# apply threshold to reclassify the predictive surface
mapPresentReclass <- reclassify(mapPresent, rcl = thresholdConditions)
plot(mapPresentReclass)
# determine model performance
getAccuracy(reducedModel,threshold = threshold)
getAUC(reducedModel, test = TRUE)
# load layers of the future
environmentalConditionsRCP26 <- stack(list.files("Data/Future RCP26", full.names = T))
# crop layers to the European extent
environmentalConditionsRCP26 <- crop(environmentalConditionsRCP26,myExtent)
# predict with BRT to raster stack
mapRCP26 <- predict(reducedModel, environmentalConditionsRCP26, type=c("logistic"))
plot(mapRCP26 - mapPresent)
# load layers of the future
environmentalConditionsRCP85 <- stack(list.files("Data/Future RCP85", full.names = T))
# crop layers to the European extent
environmentalConditionsRCP85 <- crop(environmentalConditionsRCP85,myExtent)
# predict with BRT to raster stack
mapRCP85 <- predict(reducedModel, environmentalConditionsRCP85, type=c("logistic"))
par(mfrow=c(1,3))
plot(mapPresent)
plot(mapRCP26 - mapPresent)
plot(mapRCP85 - mapPresent)
plot(mapPresent)
dev.off()
plot(mapPresent)
mapForCongress <- mapPresent
mapForCongress <- crop(mapForCongress,extent(20,30,35,50))
plot(mapForCongress)
mapForCongress <- mapPresent
mapForCongress <- crop(mapForCongress,extent(20,30,32,50))
plot(mapForCongress)
class(reducedModel)
# inspect response curves
names(environmentalConditions)
predict(reducedModel, data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=0,OceanTemperature_Benthic_Mean_Pred_LtMax=100,OceanTemperature_Benthic_Mean_Pred_LtMin=0,TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=0), type=c("logistic"))
View(data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=0,OceanTemperature_Benthic_Mean_Pred_LtMax=100,OceanTemperature_Benthic_Mean_Pred_LtMin=0,TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=0))
View(data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=c(0,220),OceanTemperature_Benthic_Mean_Pred_LtMax=c(100,4),OceanTemperature_Benthic_Mean_Pred_LtMin=c(0,16),TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=c(0,2)))
predict(reducedModel, data.frame(DissolvedMolecularOxygen_Benthic_Mean_Pred_Mean=c(0,220),OceanTemperature_Benthic_Mean_Pred_LtMax=c(100,4),OceanTemperature_Benthic_Mean_Pred_LtMin=c(0,16),TotalPrimaryProductionPhyto_Benthic_Mean_Pred_Mean=c(0,2)) , type=c("logistic"))
data.frame(from = c(-Inf,20.5) , to=c(20.5,+Inf) , reclassValue=c(1,0))
data.frame(from = c(-Inf,30) , to=c(30,+Inf) , reclassValue=c(1,0))
